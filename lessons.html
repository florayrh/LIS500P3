<!DOCTYPE html>
<html lang="en">
<head>
    <!-- 
        Metadata:
        - Sets character encoding to UTF-8 to support diverse characters.
        - Includes a viewport tag to ensure responsive design on all devices.
        - Sets the page title to "Lessons Learned" to reflect the content.
        - Links to a shared stylesheet for consistent styling across the site.
    -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lessons Learned</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <!-- 
        Navigation Bar:
        - Includes links to key sections of the website for seamless navigation.
        - Uses consistent styling across all pages to improve user experience.
    -->
    <nav>
        <a href="index.html">Home</a>
        <a href="objective.html">Project Name & Objective</a>
        <a href="machine_learning.html">What is Machine Learning</a>
        <a href="design.html">Design Process</a>
        <a href="lessons.html">Lessons Learned</a>
        <a href="try.html">Try the Project</a>
    </nav>
    <main>
        <!-- 
            Main Section:
            - Contains a heading to introduce the topic of lessons learned.
            - Includes the integrated project reflection content.
        -->
        <h1>Lessons Learned from Project</h1>
        
        <p>
            Artificial intelligence (AI) development often assumes neutrality, focusing on efficiency and technical excellence. However, Joy Buolamwini’s <i>Unmasking AI</i> challenges this assumption, exposing the deeply embedded biases within AI systems and their broader societal implications. This critical perspective influenced the development of our Medical Supplies Classifier, not as a direct application of her arguments about human recognition biases, but as a framework to examine how ethical principles in AI can apply across domains, including non-human object classification.
        </p>

        <h2>Implicit Bias and <i>Unmasking AI</i></h2>
        <p>
            Buolamwini’s <i>Unmasking AI</i> emphasizes that artificial intelligence systems are never neutral. They encode the values of their creators, reflecting both implicit and explicit biases. Implicit bias is a recurring theme in <i>Unmasking AI</i> and in course readings. 
        </p>
        <p>
            Although our classifier was not designed for facial recognition, this principle guided us to critically evaluate our dataset and development process. For example, we recognized that the medical supply images in our dataset were primarily sourced from professional online pictures, which often feature standardized, well-lit images. This created a potential bias: our model performed well in controlled environments but struggled with images captured in real-world conditions. This limitation aligns with Buolamwini’s critique of how AI systems tend to favor idealized inputs, often at the expense of marginalized users. According to Buolamwini, "Bias in AI systems isn’t only about who is excluded but also about who benefits from the design choices we make." In our case, professional images were inadvertently privileged, raising concerns about the accessibility of our classifier for users in resource-constrained environments. Acknowledging this, we expanded our dataset by including images we captured under suboptimal lighting conditions using basic cameras. However, we recognize that these efforts remain insufficient to fully address this structural imbalance.
        </p>
        <p>
            Additionally, multipurpose masks used in medicine and industry presented classification challenges because the model had difficulty reconciling ambiguous inputs. Instead of enforcing strict categories, we allowed users to define their own categories. This decision reflects Buolamwini's call for adaptive systems that "acknowledge ambiguity rather than impose false certainty."
        </p>

        <h2>Intersectionality and <i>Unmasking AI</i></h2>
        <p>
            We also considered the concept of intersectionality. <i>Unmasking AI</i> critiques how AI systems often fail to account for the overlapping dimensions of identity and power. Shapiro's critique of intersectionality also highlights the complexity of navigating overlapping identity and power structures, even in technological systems. While intersectionality typically examines human identities, we adopted this framework to explore systemic inequities in the representation of medical supplies. For example, due to cultural practices, economic factors, and regulatory standards, the design and labeling of medical supplies often vary significantly by region. In low-income regions, thermometer packaging may lack the standardized features found in wealthier areas, making it harder for the classifier to recognize these items.
        </p>
        <p>
            We believe that ignoring intersectionality reinforces existing hierarchies of power, further marginalizing those who are already on the fringes. By incorporating packaging variations from different regions, such as the United States, China, Europe, Australia, Japan, and South Korea, we aimed to make the classifier more inclusive. However, our dataset still lacked sufficient representation from certain regions, such as parts of Africa and Southeast Asia. We are deeply aware of this gap and acknowledge that it exacerbates the broader challenge of building globally equitable AI systems.
        </p>

        <h2>Transparency and Accountability</h2>
        <p>
            One of the most influential criticisms in <i>Unmasking AI</i> is the opacity of AI systems. Buolamwini describes the "black box problem," arguing that "the design of systems without transparency obfuscates accountability and makes it difficult to question their output." This insight prompted us to document the development of our classifier extensively. We detailed every decision, from data collection to model evaluation, ensuring that our approach could be carefully reviewed and improved. However, transparency is only one part of accountability. As Sensoy and DiAngelo emphasize in <i>A Reading Guide to Discussing White People</i>, "Acknowledging systemic power imbalances requires action, not just awareness." While our documentation provides a basis for critique, we lack mechanisms for real-time user feedback or interactive error reporting. This limits our ability to address bias when it arises.
        </p>

        <h2>Challenges and Lessons</h2>
        <ul>
            <li>Images with cluttered or diverse backgrounds were often misclassified, as the model prioritized clear, isolated objects.</li>
            <li>Packaging designs and labeling conventions from certain regions were insufficiently represented, limiting the classifier’s applicability in global contexts.</li>
            <li>Many medicines are packaged in boxes, making it difficult for the model to differentiate between box-packaged medicines and other box-packaged products.</li>
        </ul>
        <p>
            We also realized that optimizing for technical accuracy often conflicted with broader fairness goals, such as ensuring inclusivity across diverse image sources.
        </p>

        <h2>Beyond the Technical: The Moral Imperative of AI</h2>
        <p>
            Buolamwini’s most compelling argument is that the development of artificial intelligence is inherently a moral endeavor. She argues that the systems we create shape society and are, in turn, shaped by it. This perspective reframed our project not merely as a tool for identifying medical supplies but as a contribution to the broader discourse on responsible AI. By openly discussing the limitations of our classifier, we aim to foster critical conversations about bias, fairness, and accountability. In our context, this means designing a system that serves not only the majority but also those in marginalized positions. While our classifier is far from perfect, it represents a step toward realizing this vision.
        </p>

        <h2>Conclusion</h2>
        <p>
            The development of the Medical Supplies Classifier was both a technical challenge and an ethical journey. These lessons taught us that ethical AI is not only about achieving perfection but about committing to continuous reflection, critique, and improvement. The promise of AI lies not in its power but in our ability to wield it responsibly, with humility and justice. Our project embodies this ethos, striving to balance functionality with fairness and transparency. As we move forward, we hope to build on these lessons, contributing to a future where AI serves all of humanity, not just the privileged few.
        </p>
    </main>
    <footer>
        <!-- Footer Section: 
             - Credits the team members involved.
             - Uses consistent styling across pages.
        -->
        &copy; LIS 500 Project 3. Ruihan YANG, Wensha NIE, and Keke SONG.
    </footer>
</body>
</html>

